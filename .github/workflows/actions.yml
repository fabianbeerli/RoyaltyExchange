name: Scrape Data, build and upload Model

on:
  push:
  workflow_dispatch:
 
jobs:
  build:
    runs-on: ubuntu-latest
    steps:

      - name: checkout repo content
        uses: actions/checkout@v2 # checkout the repository content to github runner

      - name: setup python
        uses: actions/setup-python@v2
        with:
          python-version: '3.12.1' # Ensure you are using a compatible Python version

      - name: install python packages
        run: pip install -r requirements.txt

      - name: Removing previous chrome instances on runner 
        run: sudo apt purge google-chrome-stable  
        
       # Need to fetch reqs if needed
      - name: Installing all necessary packages
        run: pip install chromedriver-autoinstaller selenium pyvirtualdisplay
      - name: Install xvfb
        run: sudo apt-get install xvfb

      # Run the Scrapy spider
      - name: Scrape hikr data
        working-directory: ./spider
        run: |
          scrapy crawl gpx -s CLOSESPIDER_PAGECOUNT=50 -o file.jl

      # Upload scraped data to MongoDB
      - name: Upload data to MongoDB
        working-directory: spider/downloads
        run: |
          python ./mongo_import.py -c tracks -i ../file.jl -u "${{secrets.MONGODB_URI}}"

      # Build model
      - name: Build model
        working-directory: model
        run: |
          python ./model.py -u "${{secrets.MONGODB_URI}}"

      # Upload model
      - name: Upload model
        working-directory: model
        run: |
          python ./save.py -c "${{secrets.AZURE_STORAGE_CONNECTION_STRING}}"
