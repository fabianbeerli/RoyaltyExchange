name: Scrape Data, build and upload Model

on:
  push:
  workflow_dispatch:
 
jobs:
  build:
    runs-on: ubuntu-latest
    steps:

      - name: checkout repo content
        uses: actions/checkout@v3 # checkout the repository content to github runner

      - name: setup python
        uses: actions/setup-python@v2
        with:
          python-version: '3.9' # Ensure you are using a compatible Python version

      - name: install python packages
        run: pip install -r requirements.txt

      # Install Chrome browser and ChromeDriver
      - name: Install Chrome browser and ChromeDriver
        run: |
          sudo apt update
          sudo apt install -y wget unzip xvfb libxi6 libgconf-2-4
          wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
          sudo apt install -y ./google-chrome-stable_current_amd64.deb
          wget https://chromedriver.storage.googleapis.com/99.0.4844.51/chromedriver_linux64.zip
          unzip chromedriver_linux64.zip
          sudo mv chromedriver /usr/local/bin/
          chmod +x /usr/local/bin/chromedriver

      # Setup virtual display for headless browsing
      - name: Setup virtual display
        run: |
          Xvfb :99 -screen 0 1024x768x24 > /dev/null 2>&1 &
          export DISPLAY=:99

      # Run the Scrapy spider
      - name: Scrape hikr data
        working-directory: ./spider
        run: |
          scrapy crawl gpx -s CLOSESPIDER_PAGECOUNT=50 -o file.jl

      # Upload scraped data to MongoDB
      - name: Upload data to MongoDB
        working-directory: spider/downloads
        run: |
          python ./mongo_import.py -c tracks -i ../file.jl -u "${{secrets.MONGODB_URI}}"

      # Build model
      - name: Build model
        working-directory: model
        run: |
          python ./model.py -u "${{secrets.MONGODB_URI}}"

      # Upload model
      - name: Upload model
        working-directory: model
        run: |
          python ./save.py -c "${{secrets.AZURE_STORAGE_CONNECTION_STRING}}"
