name: Scrape Data, build and upload Model

on:
  push:
  workflow_dispatch:

jobs:
  build:
    runs-on: ubuntu-latest
    steps:

      - name: checkout repo content
        uses: actions/checkout@v3 # checkout the repository content to github runner

      - name: setup python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12.1' # install the python version needed
          cache: 'pip'
          
      - name: install python packages
        run: pip install -r requirements.txt

      - name: Prepare Chrome and ChromeDriver
        uses: browser-actions/setup-chrome@latest
        
      - name: install chromedriver
        run: |
          wget https://chromedriver.storage.googleapis.com/99.0.4844.51/chromedriver_linux64.zip
          unzip chromedriver_linux64.zip
          chmod +x chromedriver
          sudo mv chromedriver /usr/local/bin/
          chmod +x /usr/local/bin/chromedriver # Ensure chromedriver is executable

      - name: scrape hikr data 
        working-directory: ./spider
        run: scrapy crawl gpx -s CLOSESPIDER_PAGECOUNT=50 -o file.jl

      - name: upload data to mongodb
        working-directory: spider/downloads
        run: python ./mongo_import.py -c tracks -i ../file.jl -u "${{secrets.MONGODB_URI}}"

      - name: build model
        working-directory: model
        run: python ./model.py -u "${{secrets.MONGODB_URI}}"

      - name: upload model
        working-directory: model
        run: python ./save.py -c "${{secrets.AZURE_STORAGE_CONNECTION_STRING}}"
